{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents.\n",
    "- Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently\n",
    "- A document typically concerns multiple topics in different proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Application 1. Topic Modeling in Financial Documents\n",
    "\n",
    "<img src=\"graph/docflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graph/clu1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- network company\n",
    "- accounting terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Application 2: Social network analysis with topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " [**group or label** the edges and nodes in the graph based on their topic similarity](http://oak.cs.ucla.edu/~cho/papers/SIGIR12.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applicaiton 3: Discovering Health Topics in Social Media \n",
    "- [By aggregating self-reported health statuses across millions of users, characterize the variety of health information discussed in Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graph/dis.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graph/trend1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Application 4: Topic Models in Financial Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [apply topic models to financial data to obtain a more accurate view of economic networks than that supplied by traditional economic statistics.](https://web.stanford.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf)\n",
    "- The learned topic models can serve as a substitute for or a complement to more complicated network analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"graph/market.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Dirichlet allocation(LDA) - Most Popular Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- generates topics based on word frequency from a set of documents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Necesary package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- NLTK: a natural language toolkit for Python. \n",
    "- stop_words: a Python package containing stop words.\n",
    "- gensim: a topic modeling package containing our LDA model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "!pip install -U nltk\n",
    "!pip install stop-words\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Raw data\n",
    "We have following documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is  good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "and make a corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Tokenizing: converting a document to its atomic elements.\n",
    "- Stopping: removing meaningless words.\n",
    "- Stemming: merging words that are equivalent in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use NLTKâ€™s tokenize.regexp module to  segments a document into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raw_a = doc_a.lower() # transform all word into lower case\n",
    "tokens_a = tokenizer.tokenize(raw_a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stop_a = [i for i in tokens_a if not i in en_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n",
      "['brocolli', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_a)\n",
    "print(stop_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming \n",
    "Reduce words into their stems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "texts_a = [p_stemmer.stem(i) for i in stop_a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raw_b = doc_b.lower()\n",
    "tokens_b = tokenizer.tokenize(raw_b)\n",
    "stop_b = [i for i in tokens_b if not i in en_stop]\n",
    "texts_b = [p_stemmer.stem(i) for i in stop_b]\n",
    "\n",
    "raw_c = doc_c.lower()\n",
    "tokens_c = tokenizer.tokenize(raw_c)\n",
    "stop_c = [i for i in tokens_c if i not  in en_stop]\n",
    "texts_c = [p_stemmer.stem(i) for i in stop_c]\n",
    "\n",
    "raw_d = doc_d.lower()\n",
    "tokens_d = tokenizer.tokenize(raw_d)\n",
    "stop_d = [i for i in tokens_d if not i in en_stop]\n",
    "texts_d = [p_stemmer.stem(i) for i in stop_d]\n",
    "\n",
    "raw_e = doc_e.lower()\n",
    "tokens_e = tokenizer.tokenize(raw_e)\n",
    "stop_e = [i for i in tokens_e if not i in en_stop]\n",
    "texts_e = [p_stemmer.stem(i) for i in stop_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constructing a document-term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "TEXT=[]\n",
    "TEXT.append(texts_a)\n",
    "TEXT.append(texts_b)\n",
    "TEXT.append(texts_c)\n",
    "TEXT.append(texts_d)\n",
    "TEXT.append(texts_e)\n",
    "print(len(TEXT)) # TEXT is call coupus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Give ID for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perform': 22, 'brocolli': 1, 'tension': 15, 'never': 23, 'suggest': 17, 'basebal': 8, 'say': 30, 'eat': 2, 'time': 10, 'better': 29, 'feel': 28, 'often': 24, 'drive': 6, 'spend': 9, 'mother': 3, 'brother': 5, 'profession': 31, 'around': 12, 'increas': 19, 'seem': 25, 'good': 4, 'lot': 7, 'health': 20, 'blood': 14, 'school': 27, 'like': 0, 'well': 26, 'may': 16, 'expert': 13, 'caus': 18, 'practic': 11, 'pressur': 21}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replace word with ID and frequency\n",
    "- change document to bag-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in TEXT] # bow : bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(0,2), 0 represents word 'good', 2 (term frequency) is  the frequency of 'good' in the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='blue'>\"corpus\" is a document-term matrix </font> which is our starting point for LDA(topic) analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.40784451109112935), (1, 0.3581834867987973), (2, 0.11368521994734913), (3, 0.7163669735975946), (4, 0.40784451109112935), (5, 0.11368521994734913)]\n",
      "[(0, 2), (1, 1), (2, 1), (3, 2), (4, 2), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "t=models.TfidfModel(corpus=corpus)\n",
    "print(t[corpus[0]])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel =models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                   id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- num_topics : you need to decide how many topics you want to generate. \n",
    "- id2word: word id dictionary\n",
    "- passes:  you may have higher accuracy if this value is higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.086*\"health\" + 0.086*\"brocolli\" + 0.086*\"good\" + 0.061*\"eat\" + 0.037*\"caus\" + 0.037*\"may\" + 0.037*\"blood\" + 0.037*\"tension\" + 0.037*\"suggest\" + 0.037*\"expert\" + 0.037*\"increas\" + 0.037*\"say\" + 0.037*\"profession\" + 0.037*\"like\" + 0.036*\"pressur\" + 0.036*\"drive\" + 0.036*\"mother\" + 0.036*\"brother\" + 0.012*\"often\" + 0.012*\"lot\"'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topic(1,topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.098*\"health\" + 0.069*\"drive\" + 0.040*\"increas\"'),\n",
       " (1, '0.150*\"brocolli\" + 0.150*\"good\" + 0.108*\"eat\"'),\n",
       " (2, '0.059*\"brother\" + 0.059*\"mother\" + 0.059*\"pressur\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=3,  num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also can print the topic for each document(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.034067378103823549),\n",
       " (1, 0.93146216802237836),\n",
       " (2, 0.034470453873797977)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LDA model](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)  : It is damn hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is all about Bayesian \n",
    "$$P(Words|Topic)\\longrightarrow P(Topic|Words)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward thinking - how we generate document\n",
    "- Determine topics the document will cover and their percentage\n",
    "- For each topic, randomly generate a word and fill the document slot. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward tracking-backtracks and  detect topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Visualization of Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Users/neuron/anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied: numexpr in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: pytest in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: funcy in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: future in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pyLDAvis)\n",
      "Requirement already satisfied: py>=1.4.29 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pytest->pyLDAvis)\n",
      "Requirement already satisfied: python-dateutil>=2 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pandas>=0.17.0->pyLDAvis)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/neuron/anaconda3/lib/python3.5/site-packages (from pandas>=0.17.0->pyLDAvis)\n",
      "Requirement already satisfied: MarkupSafe in /Users/neuron/anaconda3/lib/python3.5/site-packages (from jinja2>=2.7.2->pyLDAvis)\n",
      "Requirement already satisfied: six>=1.5 in /Users/neuron/anaconda3/lib/python3.5/site-packages (from python-dateutil>=2->pandas>=0.17.0->pyLDAvis)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook() # which make notebook can takes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el4990946863570329195740406\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el4990946863570329195740406_data = {\"lambda.step\": 0.01, \"tinfo\": {\"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.9796983944267044, 1.97962959905111, 1.9794952096947458, 1.4135101750484873, 1.4132329230512084, 0.8469838242593969, 0.8469837042758236, 0.8469755293502459, 0.846968074068909, 0.846965472943441, 0.846962569615639, 0.8469501575312343, 0.8468761591196872, 0.8468658952183987, 0.846824775351533, 0.8468180578214449, 0.8468127587709398, 0.8468121520573244, 0.8468114395931081, 0.8467745978501956, 0.8465866956566013, 0.8457032856868427, 0.8456974836035945, 1.4036226097096554, 1.4023428733794219, 1.4020198841104394, 0.284272499853125, 0.28427098119893707, 0.28426739542562984, 0.2842648743471918, 0.5991070843690014, 0.5990994129178615, 0.5990816084863, 0.5990798302447358, 0.5990773010211152, 0.5990762298390534, 0.6102613175890604, 0.6100334972506466, 0.6091308357945766, 0.2030753651815306, 0.20307127268467073, 0.20244815985215212, 0.20231562312277943, 0.20228963681566614, 0.20228913427934542, 0.20228870633416443, 0.20228496865122786, 0.20228023044434318, 0.20225122656438646, 0.20224398692613976, 0.20219179217919397, 0.20218303732093373, 0.20218098945994914, 0.20217915475724757, 0.20217389617784487, 0.20216812999769448, 0.20216804536733407, 0.20255834940559347, 0.20236278983160214, 0.20293935139845531, 0.20284455992521977, 0.20279603513738823], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.1824944295640925, 2.18247415897633, 2.182434561093201, 1.6158729648800894, 1.6157912724568018, 1.049151869626731, 1.049151834273518, 1.0491494255280909, 1.0491472288261565, 1.0491464624033902, 1.0491456069365728, 1.0491419497104282, 1.0491201460458268, 1.0491171217827853, 1.0491050057958762, 1.0491030264726728, 1.0491014651051043, 1.0491012863366698, 1.0491010764087743, 1.049090220972975, 1.0490348555087534, 1.0487745583715133, 1.0487728487851251, 2.0127534455042317, 2.0123763706300686, 2.0122812016995, 0.8833487296921785, 0.8833482822200522, 0.8833472256703656, 0.8833464828334918, 0.8833358406207339, 0.8833390452678955, 0.8833464828334918, 0.8833472256703656, 0.8833482822200522, 0.8833487296921785, 2.0122812016995, 2.0123763706300686, 2.0127534455042317, 1.0487728487851251, 1.0487745583715133, 1.0490348555087534, 1.049090220972975, 1.0491010764087743, 1.0491012863366698, 1.0491014651051043, 1.0491030264726728, 1.0491050057958762, 1.0491171217827853, 1.0491201460458268, 1.0491419497104282, 1.0491456069365728, 1.0491464624033902, 1.0491472288261565, 1.0491494255280909, 1.049151834273518, 1.049151869626731, 1.6157912724568018, 1.6158729648800894, 2.182434561093201, 2.18247415897633, 2.1824944295640925], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.7156, -2.7156, -2.7157, -3.0525, -3.0527, -3.5646, -3.5646, -3.5646, -3.5646, -3.5646, -3.5646, -3.5647, -3.5647, -3.5648, -3.5648, -3.5648, -3.5648, -3.5648, -3.5648, -3.5649, -3.5651, -3.5661, -3.5661, -3.0595, -3.0604, -3.0606, -4.6564, -4.6564, -4.6564, -4.6564, -2.8228, -2.8228, -2.8229, -2.8229, -2.8229, -2.8229, -2.8044, -2.8047, -2.8062, -3.9047, -3.9047, -3.9078, -3.9084, -3.9086, -3.9086, -3.9086, -3.9086, -3.9086, -3.9088, -3.9088, -3.909, -3.9091, -3.9091, -3.9091, -3.9091, -3.9092, -3.9092, -3.9072, -3.9082, -3.9054, -3.9058, -3.9061], \"Term\": [\"lot\", \"around\", \"practic\", \"time\", \"basebal\", \"spend\", \"mother\", \"brother\", \"drive\", \"say\", \"profession\", \"like\", \"feel\", \"perform\", \"school\", \"seem\", \"well\", \"often\", \"better\", \"never\", \"caus\", \"expert\", \"increas\", \"suggest\", \"blood\", \"may\", \"tension\", \"eat\", \"pressur\", \"good\", \"good\", \"brocolli\", \"health\", \"pressur\", \"eat\", \"tension\", \"may\", \"blood\", \"suggest\", \"increas\", \"expert\", \"caus\", \"never\", \"better\", \"often\", \"well\", \"seem\", \"school\", \"perform\", \"feel\", \"like\", \"profession\", \"say\", \"drive\", \"brother\", \"mother\", \"spend\", \"basebal\", \"time\", \"practic\", \"lot\", \"around\", \"practic\", \"time\", \"basebal\", \"spend\", \"mother\", \"brother\", \"drive\", \"say\", \"profession\", \"like\", \"feel\", \"perform\", \"school\", \"seem\", \"well\", \"often\", \"better\", \"never\", \"caus\", \"expert\", \"increas\", \"suggest\", \"blood\", \"may\", \"tension\", \"eat\", \"pressur\", \"health\", \"brocolli\", \"good\"], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1928, 0.1928, 0.1927, 0.1565, 0.1564, 0.0763, 0.0763, 0.0763, 0.0763, 0.0763, 0.0763, 0.0763, 0.0762, 0.0762, 0.0761, 0.0761, 0.0761, 0.0761, 0.0761, 0.0761, 0.0759, 0.0751, 0.0751, -0.0701, -0.0708, -0.071, -0.8435, -0.8435, -0.8435, -0.8435, 0.9901, 0.9901, 0.9901, 0.99, 0.99, 0.99, 0.1852, 0.1848, 0.1831, -0.2634, -0.2634, -0.2668, -0.2675, -0.2676, -0.2676, -0.2676, -0.2676, -0.2677, -0.2678, -0.2679, -0.2681, -0.2682, -0.2682, -0.2682, -0.2682, -0.2683, -0.2683, -0.6982, -0.6992, -0.9969, -0.9974, -0.9977]}, \"mdsDat\": {\"Freq\": [74.80117110168196, 25.198828898318038], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"x\": [0.04654740473289251, -0.04654740473289251], \"cluster\": [1, 1]}, \"R\": 30, \"plot.opts\": {\"ylab\": \"PC2\", \"xlab\": \"PC1\"}, \"token.table\": {\"Freq\": [1.1320681513593958, 1.1320563136057455, 0.953182422855401, 0.9531530739738513, 0.9163911479887039, 0.49692493640585894, 0.49692493640585894, 0.953159865808443, 0.49683184109491446, 0.49683184109491446, 0.6188918191639354, 0.9531565431798601, 0.9532068643939445, 0.9163826367242817, 0.9164077749017052, 0.9531557659825634, 0.9532571722938864, 1.1320722583805547, 0.9531508856317703, 0.49694843799933935, 0.49694843799933935, 0.9531796751487782, 0.9531934310439936, 0.9531970012109278, 1.1320586196169833, 0.6188605303351975, 0.9534937628089986, 0.9534953170826052, 0.9531968104737291, 0.953196648047589, 1.1320557401474627, 0.953155069683456, 0.9531508535134972, 1.1320576676301977, 0.9531952294163438], \"Topic\": [2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1], \"Term\": [\"around\", \"basebal\", \"better\", \"blood\", \"brocolli\", \"brother\", \"brother\", \"caus\", \"drive\", \"drive\", \"eat\", \"expert\", \"feel\", \"good\", \"health\", \"increas\", \"like\", \"lot\", \"may\", \"mother\", \"mother\", \"never\", \"often\", \"perform\", \"practic\", \"pressur\", \"profession\", \"say\", \"school\", \"seem\", \"spend\", \"suggest\", \"tension\", \"time\", \"well\"]}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el4990946863570329195740406\", ldavis_el4990946863570329195740406_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el4990946863570329195740406\", ldavis_el4990946863570329195740406_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el4990946863570329195740406\", ldavis_el4990946863570329195740406_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x    y\n",
       "topic                                           \n",
       "1      74.801171        1       1  0.046547  0.0\n",
       "0      25.198829        1       2 -0.046547  0.0, topic_info=     Category      Freq        Term     Total  loglift  logprob\n",
       "term                                                           \n",
       "16    Default  0.000000         lot  0.000000  30.0000  30.0000\n",
       "8     Default  0.000000      around  0.000000  29.0000  29.0000\n",
       "5     Default  0.000000     practic  0.000000  28.0000  28.0000\n",
       "20    Default  0.000000        time  0.000000  27.0000  27.0000\n",
       "21    Default  0.000000     basebal  0.000000  26.0000  26.0000\n",
       "19    Default  0.000000       spend  0.000000  25.0000  25.0000\n",
       "4     Default  2.000000      mother  2.000000  24.0000  24.0000\n",
       "9     Default  2.000000     brother  2.000000  23.0000  23.0000\n",
       "11    Default  2.000000       drive  2.000000  22.0000  22.0000\n",
       "12    Default  1.000000         say  1.000000  21.0000  21.0000\n",
       "29    Default  1.000000  profession  1.000000  20.0000  20.0000\n",
       "1     Default  1.000000        like  1.000000  19.0000  19.0000\n",
       "15    Default  1.000000        feel  1.000000  18.0000  18.0000\n",
       "3     Default  1.000000     perform  1.000000  17.0000  17.0000\n",
       "13    Default  1.000000      school  1.000000  16.0000  16.0000\n",
       "24    Default  1.000000        seem  1.000000  15.0000  15.0000\n",
       "30    Default  1.000000        well  1.000000  14.0000  14.0000\n",
       "6     Default  1.000000       often  1.000000  13.0000  13.0000\n",
       "10    Default  1.000000      better  1.000000  12.0000  12.0000\n",
       "23    Default  1.000000       never  1.000000  11.0000  11.0000\n",
       "17    Default  1.000000        caus  1.000000  10.0000  10.0000\n",
       "31    Default  1.000000      expert  1.000000   9.0000   9.0000\n",
       "2     Default  1.000000     increas  1.000000   8.0000   8.0000\n",
       "18    Default  1.000000     suggest  1.000000   7.0000   7.0000\n",
       "0     Default  1.000000       blood  1.000000   6.0000   6.0000\n",
       "28    Default  1.000000         may  1.000000   5.0000   5.0000\n",
       "27    Default  1.000000     tension  1.000000   4.0000   4.0000\n",
       "25    Default  1.000000         eat  1.000000   3.0000   3.0000\n",
       "26    Default  1.000000     pressur  1.000000   2.0000   2.0000\n",
       "14    Default  2.000000        good  2.000000   1.0000   1.0000\n",
       "...       ...       ...         ...       ...      ...      ...\n",
       "5      Topic2  0.599082     practic  0.883346   0.9901  -2.8229\n",
       "20     Topic2  0.599080        time  0.883347   0.9900  -2.8229\n",
       "21     Topic2  0.599077     basebal  0.883348   0.9900  -2.8229\n",
       "19     Topic2  0.599076       spend  0.883349   0.9900  -2.8229\n",
       "4      Topic2  0.610261      mother  2.012281   0.1852  -2.8044\n",
       "9      Topic2  0.610033     brother  2.012376   0.1848  -2.8047\n",
       "11     Topic2  0.609131       drive  2.012753   0.1831  -2.8062\n",
       "12     Topic2  0.203075         say  1.048773  -0.2634  -3.9047\n",
       "29     Topic2  0.203071  profession  1.048775  -0.2634  -3.9047\n",
       "1      Topic2  0.202448        like  1.049035  -0.2668  -3.9078\n",
       "15     Topic2  0.202316        feel  1.049090  -0.2675  -3.9084\n",
       "3      Topic2  0.202290     perform  1.049101  -0.2676  -3.9086\n",
       "13     Topic2  0.202289      school  1.049101  -0.2676  -3.9086\n",
       "24     Topic2  0.202289        seem  1.049101  -0.2676  -3.9086\n",
       "30     Topic2  0.202285        well  1.049103  -0.2676  -3.9086\n",
       "6      Topic2  0.202280       often  1.049105  -0.2677  -3.9086\n",
       "10     Topic2  0.202251      better  1.049117  -0.2678  -3.9088\n",
       "23     Topic2  0.202244       never  1.049120  -0.2679  -3.9088\n",
       "17     Topic2  0.202192        caus  1.049142  -0.2681  -3.9090\n",
       "31     Topic2  0.202183      expert  1.049146  -0.2682  -3.9091\n",
       "2      Topic2  0.202181     increas  1.049146  -0.2682  -3.9091\n",
       "18     Topic2  0.202179     suggest  1.049147  -0.2682  -3.9091\n",
       "0      Topic2  0.202174       blood  1.049149  -0.2682  -3.9091\n",
       "28     Topic2  0.202168         may  1.049152  -0.2683  -3.9092\n",
       "27     Topic2  0.202168     tension  1.049152  -0.2683  -3.9092\n",
       "25     Topic2  0.202558         eat  1.615791  -0.6982  -3.9072\n",
       "26     Topic2  0.202363     pressur  1.615873  -0.6992  -3.9082\n",
       "22     Topic2  0.202939      health  2.182435  -0.9969  -3.9054\n",
       "7      Topic2  0.202845    brocolli  2.182474  -0.9974  -3.9058\n",
       "14     Topic2  0.202796        good  2.182494  -0.9977  -3.9061\n",
       "\n",
       "[92 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "8         2  1.132068      around\n",
       "21        2  1.132056     basebal\n",
       "10        1  0.953182      better\n",
       "0         1  0.953153       blood\n",
       "7         1  0.916391    brocolli\n",
       "9         1  0.496925     brother\n",
       "9         2  0.496925     brother\n",
       "17        1  0.953160        caus\n",
       "11        1  0.496832       drive\n",
       "11        2  0.496832       drive\n",
       "25        1  0.618892         eat\n",
       "31        1  0.953157      expert\n",
       "15        1  0.953207        feel\n",
       "14        1  0.916383        good\n",
       "22        1  0.916408      health\n",
       "2         1  0.953156     increas\n",
       "1         1  0.953257        like\n",
       "16        2  1.132072         lot\n",
       "28        1  0.953151         may\n",
       "4         1  0.496948      mother\n",
       "4         2  0.496948      mother\n",
       "23        1  0.953180       never\n",
       "6         1  0.953193       often\n",
       "3         1  0.953197     perform\n",
       "5         2  1.132059     practic\n",
       "26        1  0.618861     pressur\n",
       "29        1  0.953494  profession\n",
       "12        1  0.953495         say\n",
       "13        1  0.953197      school\n",
       "24        1  0.953197        seem\n",
       "19        2  1.132056       spend\n",
       "18        1  0.953155     suggest\n",
       "27        1  0.953151     tension\n",
       "20        2  1.132058        time\n",
       "30        1  0.953195        well, R=30, lambda_step=0.01, plot_opts={'ylab': 'PC2', 'xlab': 'PC1'}, topic_order=[2, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, corpus[0:4], dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.061*\"mother\" + 0.061*\"brother\" + 0.060*\"drive\" + 0.059*\"lot\" + 0.059*\"around\" + 0.059*\"practic\" + 0.059*\"time\" + 0.059*\"basebal\" + 0.059*\"spend\" + 0.020*\"say\" + 0.020*\"profession\" + 0.020*\"health\" + 0.020*\"brocolli\" + 0.020*\"good\" + 0.020*\"eat\"'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topic(0,topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## In-class practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Newlist=[]\n",
    "Newlist.append(\"4 UK Students Prove Their Amazing Money-Making System, Worth Millions, On live TV\")\n",
    "Newlist.append(\"U.S. Supreme Court may limit where companies can be sued\")\n",
    "Newlist.append(\"Wynn's New Macau Casino Delivers Forecast-Topping Profit\")\n",
    "Newlist.append(\"Gilead (GILD) Q1 Earnings: Stock Likely to Beat Estimates?\")\n",
    "Newlist.append(\"Bank of America just said there's â€˜material riskâ€™ to the long-term viability of Tesla\")\n",
    "Newlist.append(\"Is the Options Market Predicting a Spike in SeaDrill (SDRL) Stock?\")\n",
    "Newlist.append(\"Exelixis (EXEL) Q1 Earnings: Will the Stock Disappoint?\")\n",
    "Newlist.append(\"Express Scripts, Teva Pharmaceuticals Drop into Tuesdayâ€™s 52-Week Low Club\")\n",
    "Newlist.append(\"As retail bankruptcies climb toward post-recession high, these companies could be next\")\n",
    "Newlist.append(\"This Is the Eye-Opening Data That Hints the U.S. Restaurant Industry Has Completely Collapsed\")\n",
    "Newlist.append(\"Goldman has figured out the trick for making money off Amazon\")\n",
    "Newlist.append(\"T-Mobile CEO: Most people have no idea what 5G is\")\n",
    "Newlist.append(\"Is a Surprise in Store for Ford (F) this Earnings Season?\")\n",
    "Newlist.append(\"How Earned Income Affects Social Security in Retirement\")\n",
    "Newlist.append(\"EINHORN ON TESLA: 'We expect these bubbles to pop'\")\n",
    "Newlist.append(\"It's time for a reality check on Microsoft's grand turnaround vision\")\n",
    "Newlist.append(\"General Electric: Cash is Kingâ€¦and a Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(Newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT=[]\n",
    "for l in range(17):\n",
    "    raw = Newlist[l].lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stop = [i for i in tokens if not i in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stop]\n",
    "    TEXT.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gilead': 32, 'peopl': 90, 'money': 2, 'vision': 109, 'wynn': 22, 'q1': 29, 'gild': 31, '4': 8, 'million': 11, 'teva': 63, 'low': 62, 'industri': 81, 'deliv': 27, 'disappoint': 53, 'einhorn': 106, 'prove': 3, 'live': 4, 'student': 6, '52': 57, 'ceo': 88, 'like': 35, 'tesla': 39, 'limit': 14, 'su': 18, 'materi': 45, 'post': 67, 'season': 94, 'compani': 19, 'estim': 34, 'trick': 86, 'worth': 0, 'may': 16, '5g': 91, 'sdrl': 47, 'pop': 105, 'restaur': 77, 'risk': 37, 'long': 40, 'drop': 61, 'just': 46, 'high': 71, 's': 17, 'top': 28, 'can': 15, 'recess': 68, 'problem': 114, 'bankruptci': 69, 'retail': 70, 'said': 42, 'amazon': 83, 'next': 72, 'f': 96, 'realiti': 113, 'exelixi': 55, 'ford': 97, 'gener': 118, 'uk': 10, 'king': 117, 'option': 48, 'bank': 38, 'week': 58, 'collaps': 80, 'make': 5, 'retir': 102, 'profit': 26, 'suprem': 20, 'surpris': 93, 'will': 56, 'express': 66, 'hint': 75, 'idea': 87, 'forecast': 21, 'exel': 54, 'tv': 1, 'seadril': 49, 'predict': 50, 'viabil': 43, 'america': 44, 'macau': 25, 'secur': 99, 'casino': 24, 'data': 78, 'open': 79, 'microsoft': 110, 'eye': 82, 'club': 65, 'goldman': 85, 'electr': 115, 'expect': 104, 'script': 60, 'turnaround': 107, 'court': 12, 'time': 112, 'u': 13, 'tuesday': 64, 'figur': 84, 'toward': 74, 'incom': 98, 'new': 23, 'social': 101, 'store': 95, 'spike': 52, 'market': 51, 'grand': 108, 'earn': 30, 'complet': 76, 'check': 111, 'bubbl': 103, 'system': 7, 'mobil': 89, 'beat': 36, 'amaz': 9, 'pharmaceut': 59, 'stock': 33, 'term': 41, 'cash': 116, 'climb': 73, 'affect': 100, 't': 92}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in TEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel =models.ldamodel.LdaModel(corpus, num_topics=5,\n",
    "                                   id2word = dictionary, passes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.053*\"compani\" + 0.029*\"u\" + 0.029*\"high\"'),\n",
       " (1, '0.028*\"money\" + 0.028*\"make\" + 0.028*\"stock\"'),\n",
       " (2, '0.024*\"profit\" + 0.024*\"macau\" + 0.024*\"casino\"'),\n",
       " (3, '0.063*\"s\" + 0.018*\"pharmaceut\" + 0.018*\"script\"'),\n",
       " (4, '0.041*\"earn\" + 0.022*\"tesla\" + 0.022*\"stock\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=5,  num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.015387364985435362),\n",
       " (1, 0.93840886993991157),\n",
       " (2, 0.015386918619843593),\n",
       " (3, 0.015430101024563086),\n",
       " (4, 0.01538674543024654)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 UK Students Prove Their Amazing Money-Making System, Worth Millions, On live TV'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Newlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
